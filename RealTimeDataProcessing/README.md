# Scalable Real-Time Analytics System for Website Traffic

This project demonstrates a **scalable real-time analytics system** built using **Apache Kafka**, **Apache Spark**, and **Cassandra** to analyze website clickstream data in real time.

## Table of Contents
1. [Prerequisites](#prerequisites)
2. [Setup Instructions](#setup-instructions)
   - [Start Docker Environment](#1-start-the-docker-environment)
   - [Run the Kafka Producer](#2-run-the-kafka-producer-fake-user-click-generator)
   - [Verify Kafka Messages](#3-verify-kafka-messages)
   - [Run the Spark Streaming Job](#4-run-the-spark-streaming-job)
   - [Set Up Cassandra Schema](#5-set-up-cassandra-schema)
   - [Verify Data in Cassandra](#6-verify-the-data-in-cassandra)
3. [System Architecture](#system-architecture)

---

## Prerequisites

Ensure you have the following software installed:

- [Docker](https://www.docker.com/)
- [Python 3.x](https://www.python.org/)
- Apache Kafka, Apache Spark, and Cassandra will be managed within Docker containers.

---

## Setup Instructions

### 1. Start the Docker Environment

To bring up Kafka, Spark, Cassandra, and other necessary services in Docker, run:

```bash
docker compose up -d
```

This command will start all services in the background.

### 2. Run the Kafka Producer (Fake User Click Generator)

Simulate user click events by running the Kafka producer:

```bash
python clickstream_producer.py
```

This script generates fake clickstream events and sends them to Kafka.

### 3. Verify Kafka Messages

To verify that Kafka is receiving the messages, you can run the following Kafka consumer command to print messages from the website_clicks topic:

```bash
kafka-console-consumer --bootstrap-server localhost:9093 --topic website_clicks --from-beginning
```

You should see clickstream events printed in the terminal.

### 4. Run the Spark Streaming Job

Process the Kafka clickstream data using Spark Streaming by running:

```bash
python spark_streaming.py
```

This Spark job will read events from Kafka, process them, and send the processed data to Cassandra.

### 5. Set Up Cassandra Schema

Before storing data in Cassandra, set up the required keyspace and tables. First, connect to the Cassandra container:

```bash
docker exec -it <cassandra-container-id> cqlsh
```

Next, run your Cassandra setup script to create the schema:

```bash
source /path/to/scripts/cassandra_setup.cql
```

Ensure you replace /path/to/scripts/cassandra_setup.cql with the correct path to your CQL script.

### 6. Verify the Data in Cassandra

After the pipeline is running, you can query Cassandra to verify that the processed clickstream data is being stored:

```bash
SELECT * FROM clickstream_ks.clicks;
```

This query will display the clickstream events stored in the clickstream_ks.clicks table.

System Architecture

	- Kafka: Streams real-time clickstream events generated by the producer (simulated user actions).
	- Spark Streaming: Processes and enriches the events from Kafka, transforming them as necessary.
	- Cassandra: Stores the processed clickstream data for efficient querying and analysis.

This project sets up a real-time data pipeline for analyzing website traffic, providing you with a foundation to build more complex and scalable analytics systems.

Enjoy experimenting with this scalable, real-time data pipeline!

### Notes:

- This refactored README now follows Markdown best practices, including the use of proper headers, code blocks, and an organized table of contents for easier navigation.
- Be sure to replace any placeholder values (like `<cassandra-container-id>` and `/path/to/scripts/cassandra_setup.cql`) with the actual values relevant to your project.

Let me know if you need any further adjustments!